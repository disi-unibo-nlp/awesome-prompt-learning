Publication Date,Title,Acronym,Venue,PLMs,Tasks,Research Direction,Addressed Challenges,Limitations,Work Contribution 
2023-03-06,A Chinese Few-Shot Text Classification Method Utilizing Improved Prompt Learning and Unlabeled Data,CIPLUD,Applied Sciences (Switzerland),Ernie 1.0,Classification,"They propose a CIPLUD model for Chinese few-shot text classification that utilizes improved prompt learning and unlabeled data. The model consists of two modules, the Multiple Masks Optimization-based Prompt Learning (MMOPL) module and the One-Class Support Vector Machine-based Unlabeled Data Leveraging (OCSVM-UDL) module. The MMOPL module designs universal prompt templates with multiple masks for different tasks and optimizes the predicted label of the model using joint probability and length constraints. The OCSVM-UDL module assigns pseudo-labels to the unlabeled data through a one-class support vector machine model and filters noise data from the unlabeled data. The new training data are created by blending the pseudo-labeled data with the few-shot-labeled data, and the process is repeated until the performance of the text classification model stabilizes. The performance of the CIPLUD model has been effective.","Insufficiently labeled samples and low-generalization performance have become significant natural language processing problems, drawing significant concern for few-shot text classification (FSTC).  Prompt learning have improved the situation, however, prompt learning methods typically require the pre-trained language model and tokens of the vocabulary list for model training, while different language models have different token coding structures, making it impractical to build effective Chinese prompt learning methods from previous approaches related to English. In addition, a majority of current prompt learning methods do not make use of existing unlabeled data, thus often leading to unsatisfactory performance in real-world applications. ",--,Continuous prompting in framework with multiple masks
2023-04-30,A Dual Prompt Learning Framework for Few-Shot Dialogue State Tracking,--,WWW,SOLOIST,Dialogue State Tracking,"For the lack of labeled data in practical DST tasks, they focus on improving DST module to generate dialogue states in circumstances with limited annotations and knowledge about slot ontology. They design a dual prompt learning framework DPL, which considers the learning of slot generation and value generation as dual tasks and containstwo main components(value prompt and slot prompt). Their framework can efectively probe DST-related knowledge from pretrained language models and utilize it for DST task. Experiments show that their model outperforms existing state-of-the-art methods under diferent levels of resources. ","Collecting dialogue state labels including slots and values can be costly, requiring experts to annotate all (slot, value) information for each turn in dialogues. It is also difcult to defne all possible slots and values in advance, especially with the wide application of dialogue systems in more and more new-rising applications.",--,Discrete promting in framework
2023-07-01,A Medical Question Classification Approach Based on Prompt Tuning and Contrastive Learning,--,SEKE ,ERNIE 3.0,Classification,"They introduced a novel supervised contrastive learning method, pulling similar samples and pushing negative ones from different classs.","COVID-19 has profoundly impacted people's lives, and people are more concerned about medical and health issues, so it is essential to design an efficient method for classifying medical questions. Fine-tuning are the de-facto approach but are poorly robust and there is a gap between the pre-training phase and the downstream task, resulting in PLMs that cannot use the rich latent knowledge in downstream tasks","The prompt template uses manual templates, which have more parameters to optimize and are prone to overfitting",Discrete prompt with CL in framework
2023-07-27,A Prompt Learning Based Intent Recognition Method on a Chinese Implicit Intent Dataset CIID,CIID,Neural Processing Letters,BERT-base-chinese,Intent Recognition,"A prompt-based approach is proposed to identify the implicit intents in the CIID dataset and compared with several benchmark models. The experimental results show that the proposed model achieves the best performance based
on our experiments. This shows that constructing a suitable prompt for learning can enhance
the inferring ability to users’ true intents.","Most of the existing intent recognition research is limited to simple, direct, and explicit intent recognitions. However, the natural human–computer interactions are flexible and diverse, and the expressions are often the euphemistic implicit
intentions. Therefore, the implicit intent recognition brings new research challenges in this
field",--,Hard prompting in framework
2024-06-21,A Rumors Detection Method Using T5-Based Prompt Learning,PLRD,IJDWM,T5,Rumor Information Detection,"Leveraging prompt templates generated by the T5 model enhanced the model’s ability to extract information from text, effectively reducing the gap between pre-training tasks and downstream target tasks.","Accurately identifying rumor information is crucial for efficient information assessment. However, adjusting PLMs typically requires modifications for specific tasks, creating a gap between pretraining and task execution.",--,Hard prompting in framework
2023-07-19,A Unified Generative Retriever for Knowledge-Intensive Language Tasks via Prompt Learning,UGR,SIGIR,BART,Information Retrieval,"They proposed UGR, a novel Unified Generative Retriever, which can robustly serve different retrieval tasks for knowledge-intensive language tasks. To unify retrieval tasks, they formulated the retrieval problem as a conditional generation problem and introduced an n-gram-based identifier for relevant contexts at different levels of granularity. To learn different retrieval tasks with a single model, they mapped the descriptions of tasks to a few prompt tokens for keeping task specifications. Empirical results on the KILT benchmark demonstrated the superiority of the proposed method.","Task-specific retriever usually has poor generalization ability to new domains and tasks, and it may be costly to deploy a variety of specialised retrievers in practice.","Due to space limitations, we only show the results on selected datasets for each task (see table 7)",Discrete/Continuous/Hybrid prompt in framework
2024-05-17,Adapting LLMs for Efficient Context Processing through Soft Prompt Compression,SPC-LLM,ACM CMNM,Claude2,"Summarization, QA, Sentiment Analysis, Classification",The fusion of soft prompts with advanced summarization techniques presents a promising avenue for future exploration aimed at enhancing the efficiency and adaptability of LLMs.,"Effectively handling extensive contexts, crucial for myriad applications, poses a formidable obstacle owing to the intrinsic constraints of the models’ context window sizes and the computational burdens entailed by their operations.",--,Soft Prompting in framework
2023-03-28,Adaptive Prompt Learning-Based Few-Shot Sentiment Analysis,AP,Neural Processing Letters,RoBERTa-wwm-ext,Sentiment Analysis,"The proposed method can solve the problem of the prompt-dependency thanks to their hybrid approach and adaptive generation. Furthermore the adaptive layer captures the semantic information of the input text, so as to generate. 
a better prompt.","Labeled data are insufficient in many sentiment analysis tasks, and obtaining these data is time-consuming and laborious.","When the quality of the hand-crafted prompt is poor, the accuracy of the HPL method will also be very low.",Hybrid prompting in framework
2023-07-19,Adversarial Meta Prompt Tuning for Open Compound Domain Adaptive Intent Detection,AMPT,SIGIR,BERT-base,Intent Detection,"AMPT framework for intent detection in an OCDA setting, which brought the advantage of improved generalization to unseen domains. By taking advantage of the collaborative effect of meta learning, prompt tuning, and adversarial training, we learned an intent classifier that could effectively generalize to unseen open domains.","This paper takes the lead to study open compound domain adaptation (OCDA) for intent detection, which brings the advantage of improved generalization to unseen domains.",--,Continuous prompting in framework
2022-07-01,AESPrompt: Self-supervised Constraints for Automated Essay Scoring with Prompt Tuning,AESPrompt,SEKE ,BERT-base-uncased,Automated Essay Scoring,Improvement of AES using lightweight prompt tuning and self-supervised constraints,Reduce high computation costs. Limited resources available,"High variance in performance, sensitivity to text truncation in low-resource settings","Introduces AESPrompt with self-supervised constraints for AES, demonstrating competitive performance in full-data settings and significant improvement in one-shot settings"
2024-04-09,Affective Prompt-Tuning-Based Language Model for Semantic Based Emotional Text Generation ,APT-LM,IJDWM,GPT-2-Medium,Classification,"Compared with the baseline model, their model shows significant enhancement in emotional text generation in both automatic and human evaluation, while maintaining competitiveness in sentence fluency and diversity. ","The large language models based on transformers have shown strong text generation ability. However, due to the need for significant computing resources, little work has been done to generate emotional text using these models.",--,Soft Prompting in framework
2023-08-06,APICom: Automatic API Completion via Prompt Learning and Adversarial Training-based Data Augmentation,APICom,ACM Internetware,CodeT5,Recommendation Systems,"They propose a novel API completion approach APICom with prompt learning and adversarial training-based data augmentation. Specifically, APICom trains the API completion model through API prompts and adversarial examples generated by our designed method ATCom in the embedding layer.","Previous studies mainly modeled API recommendation as the recommendation task, which can recommend multiple candidate APIs for the given query, and developers may not yet be able to find what they need. Motivated by the neural machine translation research domain, we can model this problem as the generation task, which aims to directly generate the required API for the developer query.","The first external threat is related to the quality of our constructed corpus. To alleviate this threat, they use the experimental subject shared by the dataset creators, which can guarantee the generalization of our empirical findings. The second threat is they only use the mentioned dataset which only supports Java programming language.",Hard prompting in framework
2024-11-01,Application of Prompt Learning Models in Identifying the Collaborative Problem Solving Skills in an Online Task,--,CSCW,"T5, BERT, RoBERTa, GPT-2",Classification,Improving automatic coding accuracy with low-resource datasets,Improving classification accuracy for collaborative problem-solving behavioral coding,"High data preprocessing requirements, lack of context consideration in classification","Introduces a prompt-based learning pre-trained model for CPS coding, compares with other methods, and demonstrates improved performance on small datasets"
2022-12-11,ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts,ATTEMPT,EMNLP,T5,"Paraphrase Detection, Sentiment Analysis, QA, Commonsense Reasoning","Improving existing methods of multitask prompt tuning with a great trade off between task performance and efficienty, introducing an interpretable and modular task transfer.","Developing a system that allows for flexible integration of knowledge from different tasks, providing the understanding the underlying task similarities and how knowledge is transferred across tasks.","ATTEMPT, being based on prompt tuning, increases the input token length, leading to increased memory footprint and computational cost.","ATTEMPT achieves a strong balance between task performance and efficiency, outperforming existing prompt tuning methods and matching or exceeding the performance of fully fine-tuned models or other parameter-efficient methods that update significantly more parameters. It offers a modular and interpretable approach to multi-task transfer learning."
2023-05-15,Automating Method Naming with Context-Aware Prompt-Tuning,AUMENA,IEEE/ACM ICPC,CodeT5,Method Name Recommendation,"Propose AUMENA, an automated prompting method with context-aware prompts tuning. It first learns the contextualized representation(i.e., class attributes) of programming language and natural language
through the pre-training model, then fully exploits the capacity and knowledge of large language model with prompt-tuning to precisely detect inconsistent method names and recommend more accurate names. To better identify semantically consistent names, they model the method name consistency checking task as a two-class classification problem and employed an hard negative sampling method for trainingF.","Despite the promise of many automated approaches, results are still suboptimal due to: (1) the models are trained from scratch, learning two different objectives simoultaneously. (2) The enclosing class context is not fully exploited, making it difficult to learn the abstract functionality of the method. (3) Current method name consistency checking methods follow a generate-then-compare process, which restricts the accuracy as they highly rely on the quality of generated names and face difficulty measuring the semantic consistency.","Following prior studies they constructed experiments on the same datasets and, since individual knowledge varies, there is no guarantee that all method names are good enough. Also, engineers could keep their own points of view on the same naming work. Therefore, the dataset may still contain sub-optimal method names, and it can affect both the evaluations of method name recommendation and method name consistency checking tasks. Also, this approach is java limited",Discrete prompt in knowledge enhanced framework
2024-05-17,Can we Soft Prompt LLMs for Graph Learning Tasks?,GraphPrompter,WWW,LLAMA2-7B,"Node Classification, Link Prediction","They presents GraphPrompter, a novel plug-and-play framework that integrates the capabilities of Large Language Models with the structural insights of Graph Neural Networks for the task of node classification and link prediction in graph data.",Directly applying LLMs to graph modalities presents unique challenges due to the discrepancy and mismatch between the graph and text modalities,--,Soft Prompting in framework
2022-09-08,Chemical-Protein Relation Extraction with Pre-trained Prompt Tuning,--,IEEE ICHI ,"BERT, BioBERT, BlueBERT",Relation Extraction,Applying prompt tuning to improve the efficiency of pre-trained language models in biomedical relation extraction tasks,"Improving the accuracy of relation extraction, managing complex entity names",Focuses only on chemical-protein relations,Demonstrates that pre-trained prompt tuning outperforms fine-tuned PLMs in chemical-protein relation extraction
2022-06-01,Chinese Spam Detection Based on Prompt Tuning,--,SEKE ,ERNIE,Spam Detection,"Application of prompt tuning to spam detection, specifically Chinese spam emails, with an exploration of template construction.",Effective detection of disguised spam emails.,Reliance on manually designed prompt templates; future work aims to explore automatic or learnable templates.,Developed a Chinese spam detection model using ERNIE and prompt tuning.
2023-08-11,Clinical Prompt Learning With Frozen Language Models,--,IEEE Transactions on Neural Networks and Learning Systems,--,--,They investigated the viability of prompt learning on clinically meaningful decision tasks and directly compared this with more traditional finetuning methods. ,"More recent work suggests that for some tasks, directly prompting the pretrained model matches or surpasses fine-tuning in performance with few or no model parameter updates required. ","BioClinicalBERT was pretrained on MIMIC-III itself, which may in turn have made the tasks easier or inflated the evaluation performance, especially for the prompt learning approach. Task Performance Variance.",Implementation of various prompt learning methods and verbalizers for specific downstreamtask 
2022-12-07,Coherent Long Text Generation by Contrastive Soft Prompt,CSP,GEM,GPT-2,Text Generation,"Improving coherence in long text generation, especially by tackling the difficulties in planning the text generation at the semantic level and in leveraging incoherent texts during training.",Generating logical and coherent sentence sequences in long texts.,"The model still generates some incoherent texts. Addressing issues of repetitive plots, unrelated events, and conflicting logic in text generation. Leveraging the role of negative samples, particularly hard negative samples, in model training.",Proposing a novel generative model named CSP to improve coherence in long text generation
2022-02-19,Commonsense Knowledge-Aware Prompt Tuning for Few-Shot NOTA Relation Classification,CKPT,Applied Sciences,BERT,Relation Classification,Improving few-shot learning by leveraging commonsense knowledge and prompt tuning,Classifying instances that do not belong to any target categories in few-shot learning scenarios,"Current prompt tuning methods are handcrafted and straightforward, hinting at a need for more automated approaches.","Introduction of CKPT, development of a learned scoring strategy, and experiments demonstrating the effectiveness of the method"
2023-08-30,ConKgPrompt: Contrastive Sample Method Based on Knowledge-Guided Prompt Learning for Text Classification,ConKgPrompt,Electronics (Switzerland),ERNIE 3.0,Classification,"ConKgPrompt utilizes external knowledge bases for multi-granularity construction of verbalizers and refines the verbalizer based on Word2vec and similarity. Experimental results and visual analyses indicate that the ConKgPrompt method achieves better classification performance than previous studies, particularly in the few-shot settings, and outperforms existing prompt-tuning methods, demonstrating the robustness and effectiveness of ConKgPrompt.","Currently, prompt learning methods can bring state of the art (SOTA) performance to pre-trained language models (PLMs) in text classification and transform a classification problem into a masked language modeling problem. The crucial step of prompt learning is to construct a map between original labels and the label extension words. However, most mapping construction methods consider only labels themselves; relying solely on a label is not sufficient to achieve accurate prediction of mask tokens, especially in classification tasks where semantic features and label words are highly interrelated. Therefore, the accurate prediction of mask tokens requires one to consider additional factors beyond just label words. ","Datasets featuring limited data or intricate semantics showcase higher variance. This disparity underscores the susceptibility of ConKgPrompt to diverse dataset attributes, implying that there is indeed scope for further enhancement.",Hard prompting in framework with external knowledge to enhance the verbalizer.
2024-04-19,ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning,ConsPrompt,ICASSP,RoBERTa-large ,"Classification, Text Entailment","The ConsPrompt integrating prompting encoding network, Contrastive sampling module and contrastive scoring module, is able to realize multiple learning and alleviate the over-fit problem in prompt design.","Prompt has become an effective linguistic tool for utilizing pre-trained language models. However, in few-shot scenarios, subtle changes of prompt’s design always make the result widely different, and the prompt learning methods are also easy to overfit the limited samples.",--,Enhancing hard prompts performance with contrastive learning techniques.
2022-05-22,Continual Prompt Tuning for Dialog State Tracking,Continual PT,ACL,T5,Dialog State Tracking," Enhancing continual learning for dialog systems, efficient knowledge transfer between tasks","Catastrophic forgetting in continual learning, efficiency in memory and computation",--,Development of Continual Prompt Tuning
2022-11-04,Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing,CTM,IEEE  Big Data,"BERT, CharacterBERT",Entity Typing,Enhancing flexibility in entity typing and adapting to domain-specific language styles.,Handling new entities absent during training and managing unique linguistic styles of e-commerce product titles.,--,"Reformulating entity typing as a textual entailment task to handle unseen entities.
Applying continuous prompt tuning to generate optimized hypotheses.
Using fusion embeddings to address unique language styles in product titles."
2023-12-11,Contrastive Graph Prompt-tuning for Cross-domain Recommendation,PGPRec,ACM Transactions on Information Systems,LightGCN,Recommendation Systems,Their results on four cross-domain datasets showed that PGPRec efficiently leverages the prompt-tuning along with the state-of-the-art graph recommenders and achieves a competitive performance compared with the strongest baseline (SGLPT ),"Recommender systems commonly suffer from the long-standing data sparsity problem where insufficient user-item interaction data limits the systems’ ability to make accurate recommendations. This problem can
be alleviated using cross-domain recommendation techniques.  While recent crossdomain recommendation techniques used a pre-training configuration, they argue that such techniques lead to
a low fine-tuning efficiency, especially when using large neural models.",--,Hard/Soft Prompting in framework (a model-agnostic framework)
2023-09-02,ContrastNER: Contrastive-based Prompt Tuning for Few-shot NER,ContrastNER,IEEE COMPSAC,RoBERTa ,NER,"ContrastNER, a prompt-based learning framework for few-shot NER, proposes an hybrid prompt tuning to improve upon manual prompt engineering and employ a contrastive learning-based loss to unify what the prompt is learning to the verbalizer.","The strong performance of most available NER approaches is heavily dependent on the design of discrete prompts and a verbalizer to map the model-predicted outputs to entity categories, which are complicated undertakings. ",--,Hybrid prompting and verbalizers construction in framework
2022-07-01,CUP: Curriculum Learning based Prompt Tuning for Implicit Event Argument Extraction,CUP,IJCAI,BART,Argument Extraction,Enhancing implicit event argument extraction by leveraging curriculum learning and prompt tuning,Capturing long-range dependencies between arguments and triggers. Handling low-data scenarios.,--,Proposed a novel approach for implicit EAE using curriculum learning and prompt tuning
2023-04-28,Decomposed Two-Stage Prompt Learning for Few-Shot Named Entity Recognition,--,Information (Switzerland),BERT-base-cased,NER,"The proposed framework comprises an entity locating model that predicts the entity span and an entity typing model that uses prompt learning to recognize the type of predicted span using a concise yet effective prompt template. Their approach offers a promising solution to address the time-consuming problem associated with NER tasks, and the experimental results demonstrate its effectiveness.","Although prompt learning has been successfully applied in few-shot classification tasks, adapting to token-level classification similar to the NER task presents challenges in terms of time consumption and efficiency. ",--,Discrete promting in framework
2021-11-07,Discrete and Soft Prompting for Multilingual Models,--,EMNLP,XLM-RoBERTa-base,Multilingual NLI,Investigating the effectiveness of discrete and soft prompting in few-shot multilingual tasks,Improving few-shot learning performance in multilingual settings and assessing crosslingual transfer,Slightly worse performance in languages with limited pretraining data and reliance on machine-translated few-shot datasets,Systematic comparison of prompting methods versus fine-tuning for multilingual NLI tasks; proposing and evaluating mixed prompting
2022-12-11,DisCup: Discriminator Cooperative Unlikelihood Prompt-tuning for Controllable Text Generation,DisCup,EMNLP,"GPT2-large, GPT2-small",Text Generation, Improving attribute-controllable text generation while preserving text quality and reducing computational costs.,Generalization issues in vanilla prompt tuning. Lack of interpretability in control attributes. Efficient and high-quality generation with minimal computational overhead.,Limited application to fine-grained control tasks like Table-to-Text generation,Introduced unlikelihood training for better prompt optimization. Improved control performance and inference speed. Provided an alternative paradigm combining prompt-tuning and decode-time methods.
2024-01-09,Domain-Enhanced Prompt Learning for Chinese Implicit Hate Speech Detection,DePL,IEEE Access,BERT-base,Classification,"The model excelled in domain adaptability and detecting nuanced hate speech, outperforming established baselines. The work underscores the utility of domain features, as evidenced by a drop in performance when these are excluded, highlighting their integral role in model effectiveness.","In hate speech detection, previous efforts almost focused on English, leading to a notable scarcity of datasets for Hate Speech Detection in Chinese. Even more, two emerging forms of hate speech under stringent regulatory environments: 1) domain specificity, manifesting itself as nuanced and harder-to-detect proprietary aggressive rhetoric within various domains; and 2) implicitness, characterized by indirect, abstract and ambiguous cold language. This evolution presents additional complexities for Multi-domain Implicit Hate Speech Detection in Chinese.",--,Hybrid prompting in framework
2022-05-27,Dual Context-Guided Continuous Prompt Tuning for Few-Shot Learning,DCCP,ACL,RoBERTa-large,"Sentiment Analysis, Subjective Analysis, NLI, Paraphrase Detection",Improving continuous prompt tuning in few-shot learning settings.,"The paper addresses the challenges of tuning continuous prompts with only a few samples, averting dependency on hand-craft engineering and large validation samples.",--,Proposes a novel continuous prompt tuning method called DCCP that improves the performance of language models in few-shot settings.
2023-07-14,Emotion Classification on Code-mixed Text Messages via Soft Prompt Tuning,--,WASSA ,XLM-RoBERTa,Emotion Classification,"They propose prompt-tuning for emotion classification on code-mixed text messages. First they transform emoticons into textual information, then they apply a variety of templates and verbalizers to verify effectness.","Emotion classification on code-mixed text messages is challenging due to the multilingual languages and non-literal cues (i.e., emoticons)._x000D_",Most outputs from their method have only one category,discrete/continuous prompt and verbalizer in framework
2024-06-01,Enhancing Cross-Lingual Sarcasm Detection by a Prompt Learning Framework with Data Augmentation and Contrastive Learning,PDC,Electronics (Switzerland),XLM-RoBERTa ,Sarcasm Detection,They propose a framework named PDC and based on prompt learning that combines data augmentation and contrastive learning for cross-lingual sarcasm detection.,"In sarcasm, it is challenging to discern the genuine feelings behind the words. Most research mainly focuses on sarcastic texts in English, as other languages lack corpora and annotated datasets. To address the challenge of low-resource languages in sarcasm detection tasks, a zero-shot cross-lingual transfer learning method is proposed in this paper. ",The diverse forms of sarcasm make it difficult to capture some sarcastic expressions. This paper focused on knowledge transfer from English to Chinese and has not yet explored its effectiveness in other languages.,Hard prompting in framework
2022-07-01,Enhancing Entity Representations with Prompt Learning for Biomedical Entity Linking,--,IJCAI,"SAPBERT, PubMedBERT",Biomedical Entity Linking,Enhancing entity representations for improved biomedical entity linking,Variety and ambiguity in biomedical entity linking,--,Proposed a two-stage algorithm for addressing variety and ambiguity challenges; demonstrated effectiveness of prompt-tuning strategy
2024-04-11,Enhancing Zero-Shot Stance Detection with Contrastive and Prompt Learning,EZSD-CP,Entropy,"BERT-base, RoBERTa",Stance Detection,Proposed an instance-guided prompt learning method.,"Stance Detection necessitates models with robust generalization abilities to discern target-related, transferable stance features within training data. Such methods typically employ a uniform prompt pattern across all instances, yet they overlook the intricate relationship between prompts and instances, thereby failing to sufficiently direct the model towards learning task-relevant knowledge and information.",--,Soft Prompting in framework
2023-07-09,ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation,ETHICIST,ACL ,GPT-Neo 1.3B,Data extraction,"They propose ETHICIST, a method for targeted training data extraction attack. It uses soft prompts to elicit memorization in the attacked model with a smoothing loss and a calibrated confidance estimation to calibrate the scale of confidence across different samples.","PLMs have been shown to memorize a considerable fraction of their training data, leading to the privacy risk of information leackage. They aim to investigate how to recover the suffix in the data when given a suffix.","Although we conduct experiments across various model scales ranging from 125M to 6B, there are still larger language models we don’t test either because their training data is not publicly released
or because we have limited resources. Moreover, the examples in the LM-Extraction benchmark are all chosen to meet the property that there exists a prefix length (maybe longer than 50)
that causes the model to generate the suffix string exactly, which makes the extraction performance on this benchmark higher than that on randomly selected prefixes.",Continus prompt in framework
2023-06-04,Exploiting Prompt Learning with Pre-Trained Language Models for Alzheimer's Disease Detection,--,ICASSP ,"BERT, RoBERTa",Disease Detection,By incorporating disfluency features based on pause filler token frequencies into the prompt and exploiting multiple models and training strategis to perform decision voting showed promesing results,PLM domain fine-tuning is commonly based on the masked word or sentence prediction costs that are inconsistent with the back-end AD detection task.,--,Discrete prompt in framework
2024-07-18,Exploring Universal Intrinsic Task Subspace for Few-Shot Learning via Prompt Tuning,--,IEEE/ACM Transactions on Audio Speech and Language Processing,--,--,"They studied the hypothesis that PLM adaptations to various tasks can be reparameterized as optimizations within a unified low-dimensional intrinsic task subspace. They developed an analysis tool IPT. It first finds a subspace by jointly decomposing the adaptive parameters of multiple tasks and then tunes parameters within the subspace for unseen data and tasks. In experiments, they study diverse few-shot NLP tasks and demonstrate that the found subspaces contain good solutions for PLM adaptations, which is strong evidence for their hypothesis.",Why can pre-trained language models (PLMs) learn universal representations and effectively adapt to broad NLP tasks differing a lot superficially? ,"The effectiveness of IPT could still be improved, especially at low dimensions. A very in depth analysis is present in their What limits IPS? section.",Study
2023-06-04,FedPrompt: Communication-Efficient and Privacy-Preserving Prompt Tuning in Federated Learning,FedPrompt,ICASSP ,"BERT-base, RoBERTa, T5","Sentiment Analysis, Toxicity Detection, Spam Detection, Classification","FedPrompt use federated prompt tuning on decentralized data in a communication-efficient and privacy-preserving way. They employ a split aggregation method that freezing extensive PLMs parameters and only tuning and aggregating soft prompts. In this way we condense the communication costs to only 0.01% compared to PLMs, making many devices applicable for scenarios with communication constraints.",PLMs in federate learning tasks there are considerable comunication costs. The idea is to combine prompt learning with FL to explore the effects,--,Continuous prompt in framework for FL
2023-07-25,Few-shot Sentiment Analysis Based on Adaptive Prompt Learning and Contrastive Learning,APRD,Information Technology and Control,ALBERT-large,Sentiment Analysis,They propose a new sentiment analysis method by the combination of adaptive prompt learning and contrastive learning,"Although prompt-based learning has the potential to address data scarcity problems by utilizing prompts to reformulate downstream tasks, the current prompt-based methods for few-shot sentiment analysis are still considered inefficient.","the model performs well only on largescale datasets, as the dot-product attention structure fails to learn the adaptive prompt and the embedded representation in the pre-trained model simultaneously, leading to over-fitting.",Hard/Soft/Hybrid Prompting in framework
2023-08-22,Generating Chinese Event Extraction Method Based on ChatGPT and Prompt Learning,CPEE,Applied Sciences (Switzerland),BART,Event Detection,"This paper proposes a generative event extraction model CPEE based on ChatGPT and prompt learning with event extraction as the research object. The model extends and improves existing methods in two aspects: data augmentation and input construction. Firstly, it utilize ChatGPT to generate a labelled dataset for event extraction tasks and trains the model using supervised learning methods adapted to downstream tasks. Secondly, they proposes the construction of generative input templates and explicitly adds entity markers within the templates.","Regarding the scarcity of annotated data for existing event extraction tasks and the insufficient semantic mining of event extraction models in the Chinese domain, this paper proposes a generative joint event extraction model to improve existing models.",--,Hard prompting in framework
2024-08-25,Graph Intelligence with Large Language Models and Prompt Learning,--,KDD,--,--,," Comprehensive review and analysis of existing methods that integrate LLMs with graphs and prompts, offering substantial potential to enhance graph transfer capabilities across diverse tasks and domains.",--,Survey
2024-06-18,Graph‑Enhanced Prompt Learning for Personalized Review Generation,GRAPA,Data Science and Engineering ,"BERTopic, GPT-2",Text Generation,"GRAPA  further explored the historical user-item interactions and the diverse semantics of reviews. First, GRAPA extracted topic information for each review based on unsupervised clustering to model the diverse review semantics. Second, GRAPA  constructed a user-item bipartite graph with users/items as nodes and reviews as edges, and employed a heterogeneous GNN to explore the high-order historical user-item interactions to generate user/item embeddings. The user/item embeddings generated by GNN and their ID embeddings are used as prompts and fed into PLMs to guide the generation of personalized reviews. Moreover, GRAPA used contrastive learning to alleviate the interference introduced by semantically irrelevant reviews.  ","With the success of PLMs, prompt learning-based approaches have been employed to handle personalized review generation. However, the existing approach neglects the historical user-item interactions as well as the diverse semantics of the reviews (including  semantically relevant reviews and semantically irrelevant reviews).",--,Soft Prompting in framework
2024-06-14,G-SAP: Graph-based Structure-Aware Prompt Learning over Heterogeneous Knowledge for Commonsense Question Answering,G-SAP,ICMR ,"RoBERTa-large, AristoRoBERTa, RoBERTa-xlarge",Commonsense QA,"They propose G-SAP, a graph-based structure-aware prompt learning model for commonsense reasoning, which effectively tackles the over-fitting issue toward textual information and facilitates deep interaction among heterogeneous modalities of knowledge. ","Although fully fine-tuned PLM has achieved remarkable performance in commonsense reasoning, their tendency to excessively prioritize textual information hampers the precise transfer of structural knowledge and undermines interpretability.",--,Prefix tuning in framework
2024-05-13,HetGpt: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks,HetGPT,ACM,"HAN, MAGNN, HGT",Node Classification,Adapting prompt-based learning to heterogeneous graphs for semi-supervised node classification,Misalignment between pre-training and downstream tasks. Scarcity of labeled nodes leading to overfitting. ,Slow training convergence due to recalibrating all pre-trained parameters. Limitations of homogeneous graph prompting techniques when applied to heterogeneous graphs.,"First attempt to adapt the ""pre-train, prompt"" paradigm to heterogeneous graphs"
2023-07-01,HGAPT: Heterogeneous Graph Augmented Prompt Tuning for Low-Resource Fake News Detection,HGAPT,SEKE ,ERNIE,Fake News Detection,HGAPT is a prompt-based framework that leverages heterogeneous graph augmentation and augmented MASK representation making the most with the limited supervised information in low-resource settings,"Detecting low-resource fake news, particularly those pertaining to recent events that have not yet been disseminated by users and are typically in short text, remains challenging due to the lack of training data and prior knowledge. ",--,Discrete prompt in framework
2024-03-25,HGPrompt: Bridging Homogeneous and Heterogeneous Graphs for Few-Shot Prompt Learning,HGPROMPT,AAAI,"GCN, GAT, Simple-H, HAN","Node Classification, Graph Classification","They introduced HGPROMPT, bridging the gap between homogeneous and heterogeneous graphs. proposing dual-template to unify downstream tasks with pretraining irrespective of graph heterogeneity and dual-prompt to narrow the gap caused by feature and heterogeneity variations.","While there has been some early exploration of prompt-based learning on graphs, they primarily deal with homogeneous graphs, ignoring the heterogeneous graphs that are prevalent in downstream applications.",--,Graph Prompting framework
2023-07-25,Instance-Aware Prompt Learning for Language Understanding and Generation,IPL,ACM Transactions on Asian and Low-Resource Language Information Processing,"RoBERTa-large, ALBERT-xxlarge-v2. GPT-2-large","Classification, NLI, QA, Text summarization","They propose an instance-aware prompt learning method named IPL, which learns a unique prompt for each instance. IPL has the potential to be applied to both unidirectional and bidirectional PLMs on both language understanding and generation tasks. IPL outperforms all other methods and obtains the new state-of-the-art using ALBERT-xxlarge-v2.","Prompt learning has emerged as a promising new paradigm, however, the current usage of fixed prompts, whether discrete or continuous, assumes that all samples within a task share the same prompt. This assumption may not hold for tasks with diverse samples that require different prompt information. ",--,Prefix tuning in framework enhancing prompt creation
2022-11-02,Investigating Prompt Learning for Chinese Few-Shot Text Classification with Pre-Trained Language Models,--,Applied Sciences (Switzerland),"BART, RoBERTa-wwmlarge",Classification,"They propose a prompt-based Chinese text classification framework utilizing a template generation module. Furthermore, to select the most informative example for applying demonstration learning with the query sentence, we combine the cosine similarity and the mutual information and form a novel joint correlation scoring function.",Text classification tend to struggle in real-world aplications with few annotated samples. Existing prompt based methods mainly focus on english tasks and cannot adapt to chinese.,"To achieve the best performance on different tasks, the template generation module needs to be retrained on different corpus in order to generate task-specific templates, which is inefficient in real-life applications. During template evaluation, the best-performing template needs to be selected by zero-shot prediction on the validation set, which is acceptable when the sample size is small; however, it can be time-consuming in traditional text classification tasks. In order to generate high-quality templates, text-to-text pretrained models are used for fine-tuning and text generation tasks, a process that requires a high level of computer hardware.",Discrete template search in framework with demonstration candidate filtering strategy (selecting most infomrative examples from training set)
2024-05-24,KBPT: knowledge-based prompt tuning for zero-shot relation triplet extraction,KBPT,PeerJ Computer Science,"BERT-base, BERT-large",Information Extraction,Novel method called Knowledge-Based Prompt Tuning (KBPT) was proposed for zero-shot relation triplet extraction.  The proposed model effectively addresses the zero-shot setting problem.,"Knowledge representation is increasingly recognized as an effective method for information extraction. Nevertheless, numerous studies have disregarded its potential applications in the zero-shot setting.",--,Hard prompting in framework
2023-07-01,KLAPrompt: Infusing Semantic Knowledge into Pre-trained Language Models by Long-answer Prompt Learning,KLAPrompt,SEKE ,"BERT, RoBERTa, MacBERT","Classification, Semantic Textual Similarity",KLAPrompt proposes a long answer prompt-learning method incorporating semantic knowledge from the Xinhua Dictionary and splitting the answer space into several subspaces,"Prompt learning is an effective approach at leveraging PLMs for specific task without additional high computing time. However, most prompt learning methods accept one token as the answer instead of multiple tokens.",--,Continuous prompt learning with subanswer space in framework
2023-11-03,Knowledge Prompt-tuning for Sequential Recommendation,KP4SR,ACM MM,T5,Recommendation Systems,"They developed KP4SR, which is the first work that transforms KG into knowledge prompts to improve SR. By developing prompt templates and employing prompt denoising (utilizing a knowledge tree) their experiments demonstrate the effectiveness of their method. ","PLMs for SR methods still lack domain knowledge and struggle to capture users’ fine-grained preferences. Meanwhile, many traditional SR methods improve this issue by integrating side information while suffering from information loss.","Due to the input length limitation, the maximum length of the useritem interaction sequence in the previous studies was set to 5.",Hard prompting in framework
2023-06-04,Knowledge-Augmented Frame Semantic Parsing with Hybrid Prompt-Tuning,KAF-SPA,ICASSP ,T5-base,Semantic Parsing ,"They proposed KAF-SPA, a model composed by a memory-based knowledge extraction module (MKEM) and a task-oriented knowledge probing module (TKPM). MKEM selects the relevant frame knowledge from a knowledge source and constructs the continuous knowledge vector while TKPM integrates the knowledge into the prompt and frames it to the PLM as an argument identification task","Ms in semantic parsing tasks tend to favor collocated patterns presented in training data, leading to inaccurate outcomes",--,Hybrid KE prompt in framework
2024-04-18,Knowledge-Enhanced Prompt Learning for Few-Shot Text Classification,SKPT,Big Data and Cognitive Computing,RoBERTa-large,Classification,"Introduce knowledge at three stages: constructing the prompt template, constructing the prompt verbalizer","Constructing appropriate prompt templates and verbalizers remains challenging, as manual prompts often require expert knowledge, while auto-constructing prompts is time-consuming. In addition, the extensive knowledge contained in entities and relations should not be ignored. ",It's an intricate process. More effective features from external knowledge graphs or knowledge bases will enhance the performance,Hard prompting in framework
2023-03-21,Knowledge-Guided Prompt Learning for Few-Shot Text Classification,PTCKT,Electronics (Switzerland),BERT-base-chinese,Classification,"They studied how to solve the problem of text classification based on prompt learning guided by knowledge. They assumed that knowledge and text labels are highly correlated and argued that exploiting knowledge explicitly can improve the efficiency of text classification. They proposed two approaches to exploit knowledge: One is to directly encode the knowledge in the prompt template, and the other is to solve text classification and knowledge detection in a multi-task prompt model. Experiments showed that their proposed model worked well for the few-shot and zero-shot settings in most cases. ","The previous study of knowledge probing showed that the success of prompt learning contributes to the implicit knowledge stored in pre-trained language models. However, how this implicit knowledge helps solve downstream tasks remains unclear.",--,Discrete prompt in framework with external knowledge insertion
2024-10-23,Knowledge-injected Prompt Learning for Chinese Biomedical Entity Normalization,PL-Knowledge,ACM Transactions on Computer Systems,MC-BERT,Biomedical Entity Normalization,"The method uniquely combines external medical knowledge with prompt learning, significantly improving BEN performance.","Existing research falls short in tackling the more complex Chinese BEN task, especially in the few-shot scenario
with limited medical data, and the vast potential of the external medical knowledge base has not yet been fully
exploited.","Handling medical entity texts with complex data formats or relationships or with special structures. Dependency on the quality of the external knowledge base and efficient methods of
knowledge integration",Hybrid prompting in framework
2022-04-29,KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction,KnowPrompt,ACM ,RoBERTa,Relation Extraction,Enhancing relation extraction using prompt-tuning with knowledge injection,"Difficulty in determining appropriate prompts for RE, computational complexity in label word search, and incorporating semantic knowledge from relation labels",--,Proposes a novel approach for integrating knowledge into prompt-tuning for relation extraction tasks
2024-02-13,Leveraging Chain-of-Thought to Enhance Stance Detection with Prompt-Tuning,PSDCOT,Mathematics,RoBERTa-large,Stance Detection,"They propose a novel prompt-based stance detection approach, referred to as PSDCOT, which utilizes a chain-of-thought method to elicit knowledge and fuses knowledge through a multi-prompt learning network. ","Applying commondly used techniques in real-world scenarios remains challenging for several reasons. Firstly, existing methods often require the use of complex attention mechanisms to filter out noise and extract relevant background knowledge, which involves significant annotation efforts. Secondly, knowledge fusion mechanisms typically rely on fine-tuning, which can introduce a gap between the pre-training phase of pre-trained language models (PLMs) and the downstream stance detection tasks, leading to the poor prediction accuracy of the PLMs.",--,Hard prompting in framework
2024-04-15,Leveraging Pretrained Language Models for Enhanced Entity Matching: A Comprehensive Study of Fine-Tuning and PromptLearning Paradigms,--,International Journal of Intelligent Systems,--,--,"The results indicate that the soft prompt consistently out-performs other approaches across all datasets, demon-strating that generating template embeddings ina continuous space can enhance the performance of EM.","This study, for the frst time, explores the potential of using a PLM to boost the EM task through two transfer learning techniques, namely, fne-tuning andprompt learning. ",--,Investigation utilizing hard/soft prompts for EM tasks.
2023-02-27,Making Pre-trained Language Models End-to-end Few-shot Learners with Contrastive Prompt Tuning ,CP-Tuning,WSDM,"RoBERTa-large, ALBERT-large ","Sentiment Analysis, Sentence Matching, NLI, Classification","They present an end-to-end Contrastive Prompt Tuning (CP-Tuning) framework that enables few-shot learning for PLMs without designing any task-specific prompts and verbalizers. In CPTuning, we employ task-invariant continuous prompt encoding and the Pair-wise Cost-sensitive Contrastive Loss (PCCL) to train the model. Specifically, task-invariant prompt encoding eases the process of hand-crafting prompts, while PCCL learns to distinguish different classes and makes the decision boundary smoother by assigning different costs to easy and hard cases. ","In most existing approaches, the high performance of promptbased learning heavily relies on handcrafted prompts and verbalizers, which may limit the application of such approaches in real-world scenarios.",--,Continuous task-invariant prompting in framework
2023-06-26,Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-Shot Prompt Learning for Automatic Scoring in Science Education,MeNSP,AIED ,BERT,Automatic Scoring,Zero-shot approach for automatically scoring student responses based on exemplars without training. They reformed the scoring procedure as a NSP task and employed a zero grade identifier.,"NLP models to automatically score students’ written responses to science problems is critical for science education. However, collecting sufficient student responses and labeling them for training or fine-tuning NLP models is time and cost-consuming. PLMs could be adopdet with prompts but no research has employed sucha a prompt approach in science education.","The quality of the improvement is related to the characteristics of the training samples, for example, the diversity of the sample responses.",Discrete prompt in framework
2023-08-06,Medical text classification based on the discriminative pre-training model and prompt-tuning,--,Digital Health,ERNIE-Health,Classification,"By wrapping the raw input tesxt into a new input sequence with a template and calculate the probability distribution of ccandidate words, the category of a medical text can be inferred by the predicted word without using extra parameters or classiﬁers.",Classical fine-tuning approaches introduce additional parameters when training an extra classifier. The use of prompts should exploit the LLM without require the additional params.,"Their method still relies onthe quality and quantity of training data. Insufﬁcient ornoisy training data may negatively affect the performance. Secondly, their method may not be robust to noise anderrors in the input. In the real world, the input text maycontain errors or noise, which may affect the performance.Lastly, the generalizability of their model to other languagesand domains is an open research question. In their experi-ments, we focused on the Chinese language and themedical domain. However, the effectiveness of their modelin other languages and domains remains to be explored.",Discrete prompting in framework
2023-06-12,MedKPL: A heterogeneous knowledge enhanced prompt learning framework for transferable diagnosis ,--,Journal of Biomedical Informatics,BERT-base-chinese,Classification,"They propose a medical knowledge-enhanced prompt learning paradigm called MedKPL and achieve state-of-the-art classification results by incorporating medical knowledge implicitly and explicitly.  With the advantage of knowledge extraction and uniform process, their model can eliminate the difference among different sources and organize all knowledge into one representation style. The introduction of knowledge and prompt learning method exploits better few-shot and zero-shot transferability among departments. ","Each clinician now wants to have his own intelligent diagnostic partner to expand the range of services he can provide. However, the implementation of intelligent decision support systems based on clinical note has been hindered by the lack of extensibility of end-to-end AI diagnosis algorithms. Existing methods, however, cannot integrate knowledge from various knowledge sources as prompts nor can fully utilize explicit and implicit knowledge.",--,Discrete promting in framework with knowledge enhanced technique
2024-07-10,Meta Soft Prompting and Learning,--,APSIPA,BERT,Classification,"Based on the doubly-looped optimization, the learned meta learner was able to extract useful features and soft-ptompts could successfully boost a frozen pre-trained model to capture domain-specific information ",Parameter efficient learning for domain-agnostic soft prompt which is developed for few-shot unsupervised domain adaptation.,--,Soft Prompting in framework
2024-03-26,Model tuning or prompt Tuning? a study of large language models for clinical concept and relation extraction,--,Journal of Biomedical Informatics,--,--,"The prompting (e.g., hard or soft) and training strategies (e.g., frozen LLM or unfrozen LLM) are critical for performance. Training with frozen LLMs is more parameter efficient, yet, to enjoy this benefit of frozen LLMs, large LLMs with parameters over billions of parameters are required. Soft prompting with frozen LLMs is parameter efficient to adopt LLMs for patient information extraction. ",Developed a soft prompt-based learning architecture and compared 4 strategies including (1) finetuning without prompts; (2) hard-prompting with unfrozen LLMs; (3) soft-prompting with unfrozen LLMs; and (4) soft-prompting with frozen LLMs.,--,Study
2023-09-28,MsPrompt: Multi-step prompt learning for debiasing few-shot event detection,MsPrompt,Information Processing and Management,BERT-base-uncased,Event Detection,"MsPrompt few-shot approach results show that their model achieves notable performance advantages, especially in the strict low-resource scenarios, and can effectively deal with the debiasing issue for FSED.heir ","Traditional ED models are too data-hungry to accommodate real applications with scarce labeled data. Besides, typical ED models are facing the contextbypassing and disabled generalization issues caused by the trigger bias stemming from ED datasets.",--,Hard prompting in framework
2024-06-18,Multi-Relation Extraction for Cybersecurity Based on Ontology Rule-Enhanced Prompt Learning,--,Electronics (Switzerland),BERT ,NER,"Design a multi-relation extraction model based on ontology rule-enhanced prompt learning, which is a parameter-sharingbased multi-task model. Design of prompt templates combing discrete and continuous tokens and uses rule injection in prompt learning.","This paper addresses the challenges of sample scarcity, zero-shot recognition of “no relation”, and computational redundancy in the field of cybersecurity.",--,Hybrid prompting in framework
2023-05-19,Multi-Stage Prompt Tuning for Political Perspective Detection in Low-Resource Settings,--,Applied Sciences (Switzerland),RoBERTa-large,Political Bias Detection,"They proposed a novel multi-stage prompt tuning framework for political perspective detection in news media. In particular, they tune domain-specific prompts using a frozen pre-trained language model that learns the MP3 task. The experimental results confirm that their methodology significantly outperforms the strong baseline methods in few-shot and full data settings.","Political perspective detection in news media—identifying political bias in news articles—is an essential but challenging low-resource task. Prompt-based learning suffer performance degradation when the target task involves a textual domain (e.g., a political domain) different from the pre-training task (e.g., masked language modeling on a general corpus).",--,Double stage prefix tuning framework
2022-11-09,No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence,--,ESEC/FSE,--,--,"Prompt tuning can outperform finetuning under full-data settings, data scarcity settings, and crossdomain settings. More specifically, it shows great potential in low-resource scenarios, e.g., improving the BLEU scores of fine-tuning by more than 26% on average for code summarization. Their results suggest that instead of fine-tuning, they could adapt prompt tuning for code intelligence tasks to achieve better performance, especially when lacking taskspecific data._x000D_",Empirical evaluation the usage and effect of prompt tuning in code intelligence tasks instead of classical fine-tuning.,--,Experimental evaluation for code intelligence tasks
2022-06-28,Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning,"RAPT, NC-RAPT",AAAI,"GPT-2-medium, GPT-2-large",Paraphrase Generation,Investigating parameter-efficient methods for controlled paraphrase generation,Enhancing paraphrase generation quality while controlling the level of novelty,Potential performance issues on test datasets where examples are semantically distant from the training dataset,Proposing RAPT and NC-RAPT for controlled paraphrase generation with varying levels of lexical novelty
2024-03-25,On Unsupervised Domain Adaptation: Pseudo Label Guided Mixup for Adversarial Prompt Tuning,PL-Mix,AAAI,"BERT-base-uncased, RoBERTa-base",Classification,"They propose a pseudo label guided Mixup method on the adversarial prompt tuning framework, called PL-Mix, to explicitly and directionally align source data with target data and alleviate the noise delivered from  the pseudo label.","unsupervised domain adaptation  methods lack explicit control for aligning the source data and target data within the same label class, degrading the classifier’s performance in the target domain.",--,Soft Prompting in framework
2023-08-19,One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER,CP-NER,IJCAI ,T5,Cross-domain NER,"Tehy propose Cp-NER, a cross-domain NER approach utilizing collaborative domain-prefix tuning to better leverage multiple domain information.","Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. ",--,Deep prompt tuning in cross-domain framework
2022-04-25,Ontology-enhanced Prompt-tuning for Few-shot Learning,OntoPrompt,ACM,BERT-large,"Relation Extraction, Event Extraction, Knowledge Graph Completion",Enhancing few-shot learning through ontology-based prompt-tuning,"Knowledge missing, knowledge noise, and knowledge heterogeneity",Cannot handle complex or structured knowledge such as OWL reasoning rules or description logic,Proposing and evaluating OntoPrompt for improving few-shot learning tasks
2022-04-01,PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains,PADA,ACL,T5,"Rumour Detection, MNLI, Sequence Tagging","Domain adaptation, prompt learning, generalization to unseen domains","Adaptation to any possible target domain, unknown at training time",The prompt generation is limited by the set of source domains and might yield sub-optimal DRFs for unrelated target domains,"Proposing PADA, a technique for adaptation to any target domain"
2023-09-20,Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning,--,INTERSPEECH,GPT-2 medium,Dialogue State Tracking,"Utilizing soft prompt tokens to learn task-specific properties, they drastically reduce the number of parameters to less than 0.5% of prioir works and achieving better low-resource DST performance. ","Fine-tuning LM for DST is costly, growing exponentially in real-world deployment. There's a need to reduce parameter size and better utilize cross-task shared information.",Main categorized errors: (1) Hallucinations; (2) Omissions; (3) Wrong values,Continuous prompt in framework 
2023-03-23,Personalized Prompt Learning for Explainable Recommendation,PEPLER-D,ACM Transactions on Information Systems,GPT-2 ,Recommendation systems,They e propose two prompt learning approaches to exploit the rich knowledge contained in pre-trained language models for recommendation explanation generation. Their experiments demonstrate the effectiveness of their approaches in generating high-quality explanations as measured by text quality and explainability metrics.,"previous works on user-understandable recommender system mostly adopt recurrent neural networks to meet the ends, leaving the potentially more effective pre-trained Transformer models under-explored. In fact, user and item IDs, as important identifiers in recommender systems, are inherently in different semantic space as words that pre-trained models were already trained on. Thus, how to effectively fuse IDs into such models becomes a critical issue.",Whether the generated explanations possess bias or stereotype against certain groups of users. ,Discrete/Continuous prompt in framework with external knowledge insertion
2022-05-27,PPT: Pre-trained Prompt Tuning for Few-shot Learning,PPT,ACL,"T5-XXL, mT5-XXL, CPM-2",Classification,Improving efficiency and effectiveness of prompt tuning in few-shot learning scenarios,Low performance of prompt tuning in few-shot settings,--," Introduced PPT framework, provided extensive experiments, and demonstrated PPT's effectiveness over other prompt tuning baselines"
2022-10-10,PRCBERT: Prompt Learning for Requirement Classification using BERT-based Pretrained Language Models,PRCBERT,IEEE/ACM ASE,"BERT-base, BERT-large, RoBERTa-base, RoBERTa-large",Classification,Converts  one multi-class classification problem into 𝐾 binary classification problems utilizing prompt learning. In addition they collect and provide a large-scale labeled dataset (NFR-SO),"Software requirement classification is a longstanding and important problem in requirement engineering. the dataset PROMISE used by the existing approaches for this problem consists of only hundreds of requirements that are outdated according to today’s technology and market trends. Besides, the NLP technique applied in these approaches might be obsolete.","The accuracy of labels may be affected by the tags of StackOverflow itself. They did not experimented with more extensive pre-trained models. Efficiency trade off of PRCBERT, it will take more (lower than
linearly because GPU can accelerate parallel computing to a certain extent) inference time to predict the class of the input sequence.",Discrete prompt in framework
2024-08-23,Prompt learning for metonymy resolution: Enhancing performance with internal prior knowledge of pre-trained language models,PromptMR-CTC,Knowledge-Based Systems,T5-base,Metonymy Resolution,"They developed a series of prompt learning approaches and prompt-tuning augmentation strategies to further enhance the potential of prompt learning. Their experiments demonstrate that PromptMR yields state-of-the-art performance in terms of accuracy, confirming the superiority of prompt learning over the currently popular PLM-based fine-tuning techniques.","Metonymy resolution has become an important NLP task. However, common PLM fine-tuning approaches can be timeconsuming and resource-intensive, and may lead to a loss of factual prior knowledge. ",--,Hard/Soft prompt-tuning augmentation strategies
2023-07-19,Prompt Learning for News Recommendation,Prompt4NR,SIGIR,"BERT-base, RoBERTa, DeBerta",News Recommendation,"They e proposed the Prompt4NR, a prompt learning framework for news recommendation, which transforms the NR task as a cloze-task for the [MASK] prediction task.","Due to the inconsistent task objective with that of PLM, they argue that their modeling paradigm has not well exploited the abundant semantic information and linguistic knowledge embedded in the pre-training process.",--,Discrete/Continuous/Hybrid prompt in framework
2023-07-19,Prompt Learning to Mitigate Catastrophic Forgetting in Cross-lingual Transfer for Open-domain Dialogue Generation,--,SIGIR,mT5,Text Generation,They proposed a prompt learning method that preserves the multilinguality of mPLM during fine-tuning to mitigate the issue. Results on 6 languages demonstrate the effectiveness of the proposed approach.,"Dialogue systems for non-English languages have long been underexplored. More specifically, they investigated the problem of catastrophic forgetting occurred in FS-XLT and MTL in the context of dialogue generation
for non-English languages. ","Cannot provide examples for other languages (Spanish, Italian and Portuguese) due to space limitations.",Discrete tuining in framework
2023-07-30,Prompt Learning with Structured Semantic Knowledge Makes Pre-Trained Language Models Better,KLAPrompt,Electronics (Switzerland),"BERT, RoBERTa, MacBERT, MC-BERT, MedBERT, SMedBERT","Relation Extraction, Text Classification, Sentence Similarity Estimation","They propose a long-answer prompt learning method (KLAPrompt) with three different long-answer strategies (discrete answers, continuous answers, and sentence similarity) to introduce fine-grained semantic knowledge. Experimental results on five open-domain datasets demonstrate that all pre-trained language models trained with the KLAPrompt method have achieved significant improvements compared to the original PLMs. Experiments on five health-related datasets verify that pre-trained language models with the KLAPrompt method can also achieve superior performance based on the strength of the disease knowledge in MedicalKG.","The typical methods of integrating knowledge are designing different pre-training tasks and training from scratch, which requires high-end hardware, massive storage resources, and long computing times. Prompt learning is an effective approach, however most prompt learning methods accept one token as the answer, instead of multiple tokens.",--,Hard/Soft Prompting in framework for multiple token prediction
2024-03-21,Prompt Optimization in Large Language Models,HPT,Mathematics,"GPT-3, RoBERTa-large","Hypothesis Validity,  Semantic Equivalence, Sentiment Prediction, Semantic Textual Similarity, NLI",The continuous representation enables efficient exploration and exploitation over prompts using Gaussian Process-based Bayesian Optimization._x000D_,"Hard prompt optimization can be considered as a combinatorial optimization problem, however, exhaustive search is impractical; thus, an efficient search strategy is needed.",--,Hard prompt search
2024-03-25,Prompt to Transfer: Sim-to-Real Transfer for Traffc Signal Control with Prompt Learning,PromptGAT,AAAi,GPT-4,Traffc Signal Control ,"They propose a prompt-based grounded action transformation method, PromptGAT, for reinforcement learning paradigm to mitigate the sim-to-real performance gap of TSC tasks. Leveraging the inference ability of pre-trained LLMs and incorporating the perceptible domain knowledge.",In TSC performance gaps still exist when simulator-trained policies are deployed to the real world. This issue is mainly introduced by the system dynamic difference between the training simulators and the real-world environments.,--,Hard prompting in framework
2024-02-20,Prompt to Transfer: Sim-to-Real Transfer for Traffic Signal Control with Prompt Learning,PromptGAT,AAAI,GPT-4,Traffic Signal Control ,To mitigate the performance gap in sim-to-real transfer for traffic signal control.,The performance gap between policies trained in simulation and those applied in the real world due to differences in system dynamics. The difficulty in handling unobserved states in the real world.,"The work was conducted in a simulation-to-simulation environment, which might not fully reflect real-world challenges.","Introduction of PromptGAT, a method that mitigates the sim-to-real transfer problem in traffic signal control by integrating human knowledge with LLMs."
2022-05-27,Prompt Tuning for Discriminative Pre-trained Language Models,DPT,ACL,ELECTRA,"Classification, QA",Enhancing the stability and performance of tuning discriminative PLMs,"Instability in tuning large PLMs, performance gaps between pre-training and fine-tuning",Potential computational inefficiency for large-scale classification tasks,Presented the first prompt tuning framework for discriminative PLMs
2024-10-14,Prompt Tuning for Item Cold-start Recommendation,PROMO,ACM,SASRec ,Item recommendation,"The research aims to improve ""cold-start"" item recommendations using prompt tuning with positive feedback information (pinnacle feedback) and personalized prompt networks",Semantic gap between content features and recommender tasks. Model bias towards popular items due to the predominance of positive feedback from these items. Annotation costs for creating prompts,--,"Use of positive feedback (pinnacle feedback) as a prompt for ""cold-start"" items"
2022-10-14,Prompt Tuning for Multi-Label Text Classification: How to Link Exercises to Knowledge Concepts?,PTMLTC,Applied Sciences (Switzerland),"TextCNN, TagBert, BertFGM",Classification,They propose a prompt tuning method for multi-label text classification and an exercise-concept dataset. Main idea of the method is that the utilization of a selection threshold to choose the associated exercise-knowledge concept,"Existing methods often overlook automatic linking of exercises to knowledge concepts in intelligent education, a key multi-label text classification problem. In addition, these kind of tasks require large amount of training data for model optimization (time-consuming and labour-intesive). Also, a problem is the number of labelled exercises being small due to the lack of specialized expertise. ",The theshold has a relatively large impact. ,Discrete prompt in framework
2024-08-12,Prompt Tuning on Graph-Augmented Low-Resource Text Classification,G2P2,IEEE Transactions on Knowledge and Data Engineering,"BERT, RoBERTa",Classification,"Given that many text documents are related through an underlying network, they proposed a novel model called Graph-Grounded Pre-training and Prompting (G2P2). It consists of three graph interaction-based contrastive strategies in pre-training, and a prompting mechanism for the jointly pre-trained graph-text model in downstream classification. ","Low-resource text classification, with no or few labeled samples, presents a serious concern for supervised learning. Also, many text data are inherently grounded on a network structure, such as a hyperlink/citation network for online articles, and a user-item purchase network for e-commerce products. These graph structures capture rich semantic relationships, which can potentially  augment low-resource text classification.","When using the same prompts optimized for the base classes, the accuracy of G2P2 markedly diminishes when facing novel, unseen classes. Also, handcrafted prompts require significant manual labor and can give inconsistent performance on different tasks or datasets.",Hard prompting and Soft prompting in framework
2023-09-02,Prompt-Learning for Cross-Lingual Relation Extraction,Prompt-XRE,IJCNN ,mBART,Cross-Lingual Relation Extraction,"Utilization of prompt learning techniques for XRE. Various prompts aiming at discoveringa a prompt which emphasizes practicality and generalization, and avoids limiting the model to a single language. They also created a new RE dataset by extracting EnglishChinese dataset from the WMT17 En-Zh","Extending pre-trained RE models to other languages is challenging, particularly in realworld scenarios where Cross-Lingual Relation Extraction (XRE) is required. There is a limited research on the effective use of multilingual PLMs with prompts to improve XRE.",--,Various prompts for cross-lingual framework
2023-09-23,Prompt-Learning for Short Text Classification,PLST ,IEEE Transactions on Knowledge and Data Engineering,XLM-RoBERTa-large,Classification,The proposed method retrieves top N concepts from the open Knowledge Graph and conducts additional four strategies for verbalizer construction. The experiments show the effectiveness of our method.,"In the short text, the extremely short length, feature sparsity, and high ambiguity pose huge challenges to classification tasks.",--,Hard prompting in framework with knowledge enhanced verbalizer 
2024-10-14,PromSec: Prompt Optimization for Secure Generation of Functional Source Code with Large Language Models (LLMs),PromSec,CCS,"GPT-3.5 Turbo, GPT-4, CodeLlama-70B-Instruct, Bard",Secure code generation,Improving prompt-based optimization for secure and functional code generation,Security vulnerabilities in LLM-generated code and balancing security and functionality in code generation.,"Does not address all types of security vulnerabilities and requires iterative optimization, which may increase processing time","Introduces PromSec, a prompt optimization framework for secure code generation"
2024-04-04,ProRLearn: boosting prompt tuning-based vulnerability detection by reinforcement learning,ProRLearn,ASE,CodeBERT,Code Vulnerability Detection,ProRLearn improves incrementally by interacting with the environment rather than relying solely on static supervisory signals. ,"Combination of prompt tuning and reinforcement learning can ofer a potential opportunity to improve performance
in vulnerability detection. ",Multiple guiding hyperparameters. Dataset dependent learning. ,Hybrid prompting in framework
2022-11-23,PTR: Prompt Tuning with Rules for Text Classification,PTR,AI Open,RoBERTa-large,"Relation Classification, Entity Typing, Intent Classification",Improving text classification efficiency and effectiveness using structured prompts with rules.,Difficulty in manually designing prompts. Inefficiency in auto-generated and soft prompts. Bridging objective gaps in PLMs for downstream tasks.,"Heuristic rule design not optimized, future work includes auto-generating better rules. Limited exploration of integrating pre-training augmentations with PTR.",Combines structured prior knowledge with prompt tuning for efficient many-class classification.
2023-11-27,REKP: Refined External Knowledge into Prompt-Tuning for Few-Shot Text Classification,REKP,Mathematics,RoBERTa,Classification,"They proposed a refined external knowledge into a prompt-tuning (REKP) model for few-shot text classification. They enhanced the verbalizer by adding an external knowledge-expanded label vocabulary, building on prompt learning. Their approach enhances the accuracy of text classification in scenarios with limited new sample quantities, such as healthcare and finance, thereby enabling more precise downstream tasks such as decision analysis, sentiment analysis, and intention recognition.","An important problem of text classification is that the number of new text categories is growing faster than that of human annotation data, which makes many new categories of text data lack a lot of annotation data. As a result, the conventional deep neural network is forced to over-fit, which damages the application in the real world. ",--,"Hard prompting in framework with external knowledge
to enhance the verbalizer through associative extensions."
2022-07-06,Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning,RetrievalRE,ACM SIGIR,"SpanBERT, KnowBERT, LUKE, MTB ",Relation Extraction,"Approach RE as an open-book examination and propose a new semiparametric paradigm of retrieval-enhanced prompt tuning for relation extraction. They construct an open-book datastore for retrieval regarding prompt-based instance representations and corresponding relation labels as memorized key-value pairs. During inference, the model can infer relations by linearly interpolating the base output of PLM with the non-parametric nearest neighbor distribution over the datastore.",Prompt tuning methods for relation extraction may still fail to generalize on rare or hard patterns given few-shot instances.,--,Discrete prompt in knowledge enhanced framework
2023-08-09,Research on the Application of Prompt Learning Pretrained  Language Model in Machine Translation Task with Reinforcement Learning,--,Electronics (Switzerland),GPT-2,Machine Translation,Novel prompt learning method combined with reinforcement learning to optimize the performance of pretrained language models in machine translation tasks and other related applications. ,Optimization of pretrained language models is essential for specific tasks such as machine translation. This paper presents a novel approach that integrates reinforcement learning with prompt learning to enhance the performance of pretrained language models in machine translation tasks.,One of the key challenges is choosing the optimal prompt strategy and finding the right balance between the learning speed of the model and the quality of the generated outputs. ,Hard prompting in framework with reinforcement learning  
2023-10-17,Robust scientific text classification using prompt tuning based on data augmentation with L2 regularization,--,Information Processing and Management,"BERT, RoBERTa, SciBERT",Classification,They propose pairwise training on text data by using data augmentation with L2 regularization in the training  to enhance prompt tuning. ,"Their results indicate that their method surpasses existing models in terms of accuracy and robustness. Also, they demonstrate their method can also be extended to both scientific and non-scientific domains when the dataset amount is small. their methods can also extend the larger SOTA models like BERT-large-uncased, RoBERTa-large, and XLNet-large-cased.","When there's a large number of instances their method performs slightly worse than the Fine-tuning method.  Also, there are some limitations and challenges with our approach when applied to relatively large scientific text categorization datasets.",Data augmentation and l2 regularization to improve performance
2024-11-01,RPEPL: Tibetan Sentiment Analysis Based on Relative Position Encoding and Prompt Learning,RPEPL,ACM,"Tibetan-Roberta-Base, Tibetan-Alpaca-7B",Sentiment Analysis,Improving sentiment analysis in low-resource languages using prompt learning and relative position encoding,"Low-resource Tibetan NLP, difficulty in word segmentation, lack of annotated data","Model performance is dependent on the size and quality of training data, Large models improve accuracy but increase computational cost","Introduces a novel Tibetan sentiment analysis method using prompt learning, incorporates relative position encoding for improved contextual understanding and enhances word representation in syllable sequences using a squashed lattice structure"
2023-08-09,Simple Knowledge Graph Completion Model Based on Differential Negative Sampling and Prompt Learning,--,Information (Switzerland),T5-base,Knowledge Graph Completion,"They identified the critical factor for the suboptimal performance as the negative sampling process. To tackle this issue, they harnessed the structure information of the knowledge graph for categorization. They then adopted corresponding negative sampling methods for different categories, thereby proposing a novel PLM-based KGC model. ","The incompleteness of existing KGs hinders their effectiveness in practical applications. . Currently, embedding-based techniques dominate the field as they leverage the structural information within KGs to infer and complete missing parts. Nonetheless, these methods exhibit limitations. They are limited by the quality and quantity of structural information and are unable to handle the missing entities in the original KG. To overcome these challenges, researchers have attempted to integrate pretrained language models and textual data to perform KG completion. However, text-based methods still lag behind embedding-based models in terms of performance.",--,Hard prompting in framework
2023-07-19,Soft Prompt Decoding for Multilingual Dense Retrieval,KD-SPD,SIGIR,"XLM-R , mBERT",Multilingual Information Retrieval ,"They presented a knowledge distillation (KD) framework based on soft prompt decoding (SPD) to address the multilingual information retrieval (MLIR) task. Using the soft prompt matrix as a task indicator, KD-SPD can implicitly translate documents from multiple languages into the same embedding space as the query language. Their experimental results show that KD-SPD significantly outperforms other baselines on three qualitatively different MLIR evaluation datasets. ",SOTA approaches in MLIR lead to sub-optimal performance. This is due to the heterogeneous and imbalanced nature of multilingual collections – some languages are better represented in the collection and some benefit from large-scale training data. ,--,Continuous prompting in framework
2023-10-20,Soft Prompt Enhanced Joint Learning for Cross-domain Aspect-based Sentiment Analysis,--,Intelligent Systems with Applications,T5-base,Sentiment Analysis,"Their model can achieve good performance, indicating that their method is effective to tackle cross-domain aspect term extraction task on both small-scale and
large-scale datasets.","Aspect term extraction is a fundamental task in fine-grained sentiment analysis. Traditional supervised models have achieved promising results with annotated datasets. However, their performance dramatically decreases in cross-domain aspect term extraction tasks. Existing cross-domain transfer learning methods face two common limitations: (1) these works directly inject linguistic features into language models, making it challenging to transfer linguistic knowledge to the target domain; (2) they rely on the fixed predefined prompts, which is time-consuming to construct the prompts for all potential aspect term spans. ",--,Soft prompt in framework 
2024-02-04,SPeC: A Soft Prompt-Based Calibration on Performance Variability of Large Language Model in Clinical Notes Summarization,SPeC,Journal of Biomedical Informatics,"Flan-T5, BART,  Pegasus-xsum",Summarization,"By employing soft prompts with discrete prompts, our approach effectively mitigates the variance in summarizing clinical notes while still harnessing the benefits of promptbased summarization techniques.","Manual summarization of clinical findings and reports into summaries is both time-consuming and prone to errors. Moreover, given the volume and complexity of the data, even experienced clinicians can inadvertently overlook significant aspects of the patient’s condition. Thus, there is a pressing need to develop automated methods for generating summaries, enhancing both efficiency and accuracy in patient care.","Complex medical terminologies and abbreviations might appear in other medical reports, which can lead to vague or misleading summaries when the model attempts to generate concise information regarding non-radiology reports. Some medical reports may reveal more private and demographic information about the patients for necessity.  Ethically, solely  relying on AI-generated summaries might lead to reduced human oversight, potentially overlooking critical findings or errors that only a human expert could detect.",Hybrid prompting in framework
2022-05-27,The Power of Prompt Tuning for Low-Resource Semantic Parsing,--,ACL,"T5, BART",Semantic Parsing,Adapting large pre-trained language models to low-resource semantic parsing tasks using prompt tuning,Effective adaptation of language models to low-resource setups. Canonical versus meaning representations for semantic parsing. Large model capability to handle out-of-distribution data.,Requires significantly more training epochs than fine-tuning. Lacks extensive analysis of learned prompts. Training with meaning representations is compute-intensive.,Demonstrates that prompt tuning outperforms fine-tuning in low-resource semantic parsing. 
2024-08-20,TopicTag: Automatic Annotation of NMF Topic Models Using Chain of Thought and Prompt Tuning with LLMs,TopicTag,DocEng,"Mistral-7BInstruct-v0.2, Mistral-7B-Instruct-v0.3, MetaLlama-3-8B-Instruct, Meta-Llama-3-70B-Instruct, GPT-4o.",Topic Labeling,Their TopicTag algorithm can effectively generalize in-domain to provide concise label summarizations for document clusters. This is achieved by incorporating the outputs of NMFk as document features within LLM prompts. It still is an underexplored research area.,"Automating topic labeling in documents clustered via NMF with automatic model determination (NMFk). By
leveraging the output of NMFk and employing prompt engineering, idea to utilize large language models (LLMs) to generate accurate topic labels. ",--,Hard prompting in framework
2022-08-14,Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning,UniCRS,KDD,DialoGPT-small,Conversational Recommender Systems,"UniCRS, a knowledge-enhanced prompt approach, unifies the recommendation and conversation subtasks into the prompt learning paradigm. It utilizes knowledge-enhanced prompts based on a fixed PLM to
fulfill both subtasks in a unified approach, infusing task-specific knowledge soft tokens with the dialouge history input. They also fused the KGs and model semantic space to solve their gap.","Existing CRS works either design semantic alignment strategies, or share knowledge resources and representations between the two modules. These approaches rely on different architectures or techniques to develop the two modules, making it difficult for effective module integration.",--,Continuous prompt tuning in KE framework
2023-09-20,Towards using Few-Shot Prompt Learning for Automating Model Completion,--,ICSE NIER,GPT-3 text-davinci-002,Domain Modeling Completion,"One advantage of their approach is the ability to target different modeling formalisms by defining semantic mapping between the formalism constructs and the natural language concepts. Additionally, the semantic mapping is illustrated with few examples, i.e., few shots, to help the language model find good results for a given prompt.",Developments in deep learning have opened a world of possibilities to automate and assist software specialists in software development and amintenence. These opportunities are limited when dealing with early software development phases such as analysis and design.,A calibration study is still necessary to determine the boundaries of the provided existing context to have the best suggestions. Another consideration that has to be studied is the use of non-natural language elements such as symbols and digits. They believe that a more sophisticated mapping of those elements would considerably improve the results.,Discrete prompting in framework
2023-05-18,"Translating radiology reports into plain language using ChatGPT and GPT-4 with prompt learning: results, limitations, and potential",--,"Visual Computing for Industry, Biomedicine, and Art",ChatGPT with GPT-4,Text Simplification,ChatGPT’s plain language translation tends to oversimplify or overlook some key points when using an ambiguous prompt.Such uncertainty can be reduced  by replacing the vague prompt with an optimized prompt.,"In this study, we investigate the feasibility of using ChatGPT in experiments on translating radiology reports into plain language for patients and healthcare providers so that they are educated for improved healthcare.","The uncertainty in ChatGPT’s responses. Given the same prompt for the same radiology report, ChatGPT generates distinctive responses each time, which results in a variety of translated reports. Such random results are partially inherent to the language model and partially attributed to the ambiguity of the prompts.",Study
2023-07-19,VPN: Variation on Prompt Tuning for Named-Entity Recognition,VPN,Applied Sciences (Switzerland),BERT-base-chinese,NER,"They propose a simple yet effective variation on prompt tuning for Chinese NER. We took the one-pass decoding strategy, which significantly increases the decoding speed. They let the LM predict several label words derived from a training dataset and convert them to label tokens in the vocabulary of the pre-trained model, retrieving the overall tag-related logits. For small datasets, such as Weibo, compared to the vanilla BERT-tagger, the rest of the baseline models have little improvement, while their model improved by 4–5%","Despite success of prompt-based methods in sentence-level classification tasks, these methods work poorly in token-level tasks, such as named entity recognition (NER), due to the sophisticated design of entity-related templates. The template design is complex for NER. To obtain templates, NER needs to enumerate all possible entity spans and types, then feed the spans and types to a pre-defined template, which is time-consuming and labour-intensive.",--,Discrete promting in framework
2024-05-23,You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content,--,IEEE SP,--,--,"They found that prompt tuning can achieve comparable or even better performance compared to the baselines. As shown in their evaluation, integrating prompt tuning into toxic content research can better help to improve the dataset quality and the model utility as the toxicity label (Task 1), predicted toxic span (Task 2), and detoxified sentence (Task 3) can be used to assist the labeling procedure._x000D_","Research focuses on developing solutions to detect toxic content, usually leveraging machine learning (ML) models trained on human-annotated datasets. While these efforts are important, these models usually do not generalize well and they can not cope with new trends.","Conducting experiments with larger models with billions of parameters would be appealing, our hardware capabilities limit such endeavors. Also, they use Perspective API as an indicator to quantify the toxicity level (e.g., on Task 3), which is likely to yield some false positives/false negatives.","systematic
analysis focusing on how prompt learning can help tackle
the problem of toxic content."
2023-06-26,Zero-Shot Cross-Lingual Event Argument Extraction with Language-Oriented Prefix-Tuning,LAPIN ,AAAI,mT5,"Event Argument Extraction, Recommendation Systems, Dialogue Systems, Summarization",Zero-shot cross-lingual event argument extraction,Differences in languages regarding the distance between triggers and arguments,--,Proposal of LAPIN for zero-shot cross-lingual EAE
