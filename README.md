# awesome-prompt-learning

This is a curated list of papers about Prompt Learning in natural language processing (NLP).

This list serves as a complement to the survey below.

## Paper

- [ACL 2021] Prefix-Tuning: Optimizing Continuous Prompts for Generation [[pdf]](https://aclanthology.org/2021.acl-long.353.pdf) [[code]](https://github.com/XiangLi1999/PrefixTuning)

- [ACL 2021] The Power of Scale for Parameter-Efficient Prompt Tuning [[pdf]](https://aclanthology.org/2021.emnlp-main.243.pdf) [[code]](https://github.com/google-research/prompt-tuning)

- [ACL 2022] P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks [[pdf]](https://aclanthology.org/2022.acl-short.8.pdf) [[code]](https://github.com/THUDM/P-tuning-v2)

- [arXiv 2022] P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks [[pdf]](https://arxiv.org/pdf/2110.07602) [[code]](https://github.com/THUDM/P-tuning-v2)

- [ACL 2022] SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer [[pdf]](https://aclanthology.org/2022.acl-long.346.pdf) [[code]](https://github.com/google-research/prompt-tuning/tree/main/prompt_tuning/spot)

- [ACL 2022] ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts [[pdf]](https://aclanthology.org/2022.emnlp-main.446.pdf) [[code]](https://github.com/AkariAsai/ATTEMPT)

- [arXiv 2023] Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning [[pdf]](https://arxiv.org/pdf/2303.02861) [[hf]](https://huggingface.co/docs/peft/package_reference/multitask_prompt_tuning)

- [arXiv 2024] DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning [[pdf]](https://arxiv.org/pdf/2309.05173) [[code]](https://github.com/zhengxiangshi/dept)

## Survey
